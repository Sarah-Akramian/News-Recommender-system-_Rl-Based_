{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer,one_hot\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# import string\n",
    "from tensorflow.keras.layers import  Embedding,Dense,Conv1D,Input,Flatten,LeakyReLU,Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# اخبار"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"E:\\\\every_thing_about_python\\\\rahnamaKalej\\\\machine learning bootcamp\\\\DataSets\\\\MINDlarge_train\\\\news.tsv\"\n",
    "        ,sep='\\t',header=None,)\n",
    "\n",
    "news.columns  =[\"NewsID\"\n",
    "        ,\"Category\",\"SubCategory\",\"Title\",\"Abstract\"\n",
    "            ,\"URL\",\"Title Entities\",\"Abstract Entities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# کاربران"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"E:\\\\every_thing_about_python\\\\rahnamaKalej\\\\machine learning bootcamp\\\\DataSets\\\\MINDlarge_train\\\\behaviors.tsv\"\n",
    "        ,sep='\\t',header=None,)\n",
    "\n",
    "users.columns = [\"impression ID\",\"UserID\",\"Time\",\"History\",\"Impressions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (فیلتر اول) ساخت کلاس برای لایه‌ی اول معماری"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Favorite_Categories():\n",
    "\n",
    "    def __init__(self , users ,news):\n",
    "\n",
    "        \n",
    "        self.news                   = (news.copy(deep=True)).set_index(\"NewsID\")\n",
    "\n",
    "\n",
    "\n",
    "        self.users                  = users.copy(deep=True)\n",
    "        self.number_of_categoeies   = len(set(self.news.Category))\n",
    "        \n",
    "        self.users.fillna(\" \",inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.category_of_news       = list(set(self.news.Category))\n",
    "\n",
    "\n",
    "    def get_favorite_categories_in_term(self,userid):\n",
    "\n",
    "        #  تاریخچه ی اخبار مشاهده شده توسط کاربر\n",
    "        \n",
    "        history                     = \" \".join(set(self.users[self.users.UserID==f\"U{userid}\"].History)).split()\n",
    "    \n",
    "\n",
    "        category_history            = [ self.news.loc[news].Category    for news in history]\n",
    "        category_history            = ' '.join(category_history)\n",
    "        \n",
    "        favorite_dict               ={ f\"{category}\":category_history.count(category) \n",
    "                                        for category in self.category_of_news if category in category_history }\n",
    "\n",
    "\n",
    "        if favorite_dict:\n",
    "            \n",
    "            #  در این لیست ، دسته ی اخباری که مشاهده شده اند، به ترتیب بیشترین بازدید تا کمترین بازدید چیده می شوند\n",
    "            favorit_list            =[]\n",
    "\n",
    "            while favorite_dict:\n",
    "\n",
    "                max_seen_category   = max( favorite_dict ,key=favorite_dict.get )\n",
    "                favorite_dict.pop(max_seen_category)\n",
    "\n",
    "                favorit_list.append(max_seen_category)\n",
    "\n",
    "            return favorit_list\n",
    "        \n",
    "        else:\n",
    "\n",
    "\n",
    "            return  self.category_of_news\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "Filter1 = Favorite_Categories(users ,news)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL_RS تعریف محیط برای لایه‌ی دوم معماری "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondFilterEnvironment():\n",
    "                                    \n",
    "    def __init__(self,users , news ,max_news_features=5000):\n",
    "        \n",
    "        # max_feature_news ==> در یک متن پیدا میکند Tfidf حداکثر تعداد لغاتی که بردار \n",
    "\n",
    "        # تمامی کاربران\n",
    "        self.users = users.copy(deep=True)\n",
    "        # خالی قرار داده میشود  String  یک  NAN برای کاربرانی که تاریخچه ای ندارند، به جای \n",
    "        self.users.History.fillna(\" \",inplace=True)\n",
    "        \n",
    "        self.news = (news.copy(deep=True)).set_index(\"NewsID\")\n",
    "        self.number_of_categoeies = len(set(self.news.Category))\n",
    "\n",
    "        #  تمامی کلماتی که برای پردازش نیاز داریم \n",
    "        self.AllWords =\" \".join(set(self.news.Category)) + \" \".join(set(self.news.SubCategory)) + \" \".join(self.news.Title)\n",
    "\n",
    "        self.NewsToArray = TfidfVectorizer(max_features=max_news_features,stop_words=\"english\")\n",
    "        self.NewsToArray.fit([self.AllWords])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "    # DO NOT USE THIS METHOD!!!!!!!!!!!!!!!!!!!!!!!!!!!!   \n",
    "    # this is a private method that gives state  in vector form and Text form\n",
    "    def __get_state(self,userid ):\n",
    "\n",
    "        UserHistory     = \" \".join(set(self.users[self.users.UserID==f\"U{userid}\"].History)).split()\n",
    "        \n",
    "        Category        = \"\"\n",
    "        \n",
    "        SubCategory     = \"\"\n",
    "\n",
    "        Title           = \"\"\n",
    "        \n",
    "        for news in UserHistory:\n",
    "\n",
    "            Category   += \" \" + self.news.loc[news].Category\n",
    "            SubCategory+= \" \" + self.news.loc[news].SubCategory\n",
    "            Title      += \" \" + self.news.loc[news].Title\n",
    "\n",
    "        # با کمک این بردار میتوان به ویژگی های متون خبری ای که کاربر قبلا مشاهده کرده دست یافت\n",
    "        UserFeaturesText = Category +\" \"+ SubCategory +\" \"+ Title\n",
    "        UserFeaturesArray = self.NewsToArray.transform([UserFeaturesText])\n",
    "        UserFeaturesArray = UserFeaturesArray.toarray()*100\n",
    "\n",
    "\n",
    "        return UserFeaturesText , UserFeaturesArray\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # DO NOT USE THIS METHOD!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # this is a private method \n",
    "    def __get_state_actionReward_newState(self,userid ):\n",
    "        \n",
    "        StateText ,StateVector                    = self.__get_state(userid)\n",
    "\n",
    "\n",
    "        #  تمامی اخباری که به کاربر پیشنهاد شده است.\n",
    "        Impressions                               = \" \".join(self.users[self.users.UserID==f\"U{userid}\"].Impressions).split()\n",
    "\n",
    "        #  تعریف یک دیکشنری برای این که مشخض شود هر کدام از اخبار پیشنهادی به کاربر، دیده شده است یا خیر\n",
    "        ImpressionsDict                           = { im.split(\"-\")[0]:int(im.split(\"-\")[-1])     for im in Impressions}\n",
    "        # /////////////////////\n",
    "        \n",
    "        # به روز رسانی گذشته ی کاربر\n",
    "        NewStateText                              =  \"\"\n",
    "        NewStateText                              += StateText\n",
    "\n",
    "        LikedNewsText                             =  \"\"\n",
    "        DisLikedNewsText                          =  \"\"\n",
    "\n",
    "        Reward                                    =  19\n",
    "        PunishMent                                =  -8\n",
    "\n",
    "        #  به ازای هر خبری که مشاهده شده 19 امتیاز مثبت و به ازای هر خبری که مشاهده نشده 8 امتیاز منفی دریافت میشود\n",
    "        RecievedRewards                           = {\"Punishment\":0 ,\"Reward\":0}\n",
    "\n",
    "        for news,seen in ImpressionsDict.items():\n",
    "            \n",
    "            Category                              = self.news.loc[news].Category\n",
    "            Subcategory                           = self.news.loc[news].SubCategory\n",
    "            Title                                 = self.news.loc[news].Title\n",
    "\n",
    "            NewsFeatures                          = Category + Subcategory + Title\n",
    "\n",
    "            #به روز رسانی میشود newstate اگر کاربر خبر را مشاهده کرده باشد، پس بردار مربوط به ویژگی های کاربر به در غالب \n",
    "            # اضافه میشود  LikedNews  همچنین کلمات استفاده شده در این خبر برای کشف ویژگی های خبر مورد علاقه ی کاربر به متغیر\n",
    "            if seen==1:\n",
    "                NewStateText                      += \" \"      + NewsFeatures\n",
    "                LikedNewsText                     += \" \"      + NewsFeatures \n",
    "                RecievedRewards[\"Reward\"]         += Reward\n",
    "            \n",
    "            else:\n",
    "                DisLikedNewsText                  += \" \"        + NewsFeatures\n",
    "                RecievedRewards[\"Punishment\"]     += PunishMent\n",
    "\n",
    "        # گذشته ی به روز رسانی شده ی کاربر\n",
    "        NewStateVector                             = self.NewsToArray.transform([NewStateText])\n",
    "        NewStateVector                             = NewStateVector.toarray()*100\n",
    "        # ویژگی های متون خبری ای که کاربر دوست داشته\n",
    "        LikedNewsVector                            = self.NewsToArray.transform([LikedNewsText])\n",
    "        LikedNewsVector                            = LikedNewsVector.toarray()*100\n",
    "        # ویژگی های متون خبری ای که کاربر دوست نداشته\n",
    "        DisLikedNewsVector                         = self.NewsToArray.transform([DisLikedNewsText])\n",
    "        DisLikedNewsVector                         = DisLikedNewsVector.toarray()*100\n",
    "\n",
    "        Result                                     ={ \"state\":StateVector , \"LikedNewsVector\":LikedNewsVector ,\n",
    "                                                      \"DisLikedNewsVector\":DisLikedNewsVector ,\"newstate\":NewStateVector  }\n",
    "        \n",
    "        \n",
    "        return {**Result , **RecievedRewards}\n",
    "    \n",
    "\n",
    "    #  ONLY USE THIS METHOD*****************************\n",
    "    def get_transitions(self,userid):\n",
    "\n",
    "        Transitions =  self.__get_state_actionReward_newState(userid)\n",
    "\n",
    "        return Transitions\n",
    "\n",
    "\n",
    "\n",
    "env = SecondFilterEnvironment(users , news ,5000)\n",
    "\n",
    "\n",
    "\n",
    "del users , news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   ساخت مدل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " User Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " News Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 1, 128)       640128      ['User Features[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 1, 128)       640128      ['News Features[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_40 (LeakyReLU)     (None, 1, 128)       0           ['conv1d_16[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_42 (LeakyReLU)     (None, 1, 128)       0           ['conv1d_17[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_16 (Flatten)           (None, 128)          0           ['leaky_re_lu_40[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_17 (Flatten)           (None, 128)          0           ['leaky_re_lu_42[0][0]']         \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 128)          16512       ['flatten_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 128)          16512       ['flatten_17[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu_41 (LeakyReLU)     (None, 128)          0           ['dense_32[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_43 (LeakyReLU)     (None, 128)          0           ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 256)          0           ['leaky_re_lu_41[0][0]',         \n",
      "                                                                  'leaky_re_lu_43[0][0]']         \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 256)          65792       ['concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_44 (LeakyReLU)     (None, 256)          0           ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 2)            514         ['leaky_re_lu_44[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,379,586\n",
      "Trainable params: 1,379,586\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " User Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " News Features (InputLayer)     [(None, 1, 5000)]    0           []                               \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 1, 128)       640128      ['User Features[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 1, 128)       640128      ['News Features[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_45 (LeakyReLU)     (None, 1, 128)       0           ['conv1d_18[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_47 (LeakyReLU)     (None, 1, 128)       0           ['conv1d_19[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_18 (Flatten)           (None, 128)          0           ['leaky_re_lu_45[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_19 (Flatten)           (None, 128)          0           ['leaky_re_lu_47[0][0]']         \n",
      "                                                                                                  \n",
      " dense_36 (Dense)               (None, 128)          16512       ['flatten_18[0][0]']             \n",
      "                                                                                                  \n",
      " dense_37 (Dense)               (None, 128)          16512       ['flatten_19[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu_46 (LeakyReLU)     (None, 128)          0           ['dense_36[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_48 (LeakyReLU)     (None, 128)          0           ['dense_37[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 256)          0           ['leaky_re_lu_46[0][0]',         \n",
      "                                                                  'leaky_re_lu_48[0][0]']         \n",
      "                                                                                                  \n",
      " dense_38 (Dense)               (None, 256)          65792       ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_49 (LeakyReLU)     (None, 256)          0           ['dense_38[0][0]']               \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 2)            514         ['leaky_re_lu_49[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,379,586\n",
      "Trainable params: 1,379,586\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(StateShapeUser,StateShapeNews):\n",
    "    \n",
    "\n",
    "    UserFeatures = Input(shape=StateShapeUser,name=\"User Features\")\n",
    "\n",
    "    NewsFeatures = Input(shape=StateShapeNews,name=\"News Features\")\n",
    "\n",
    "    user         = Conv1D(128,1,input_shape=(1,5000))(UserFeatures)\n",
    "    user         = LeakyReLU()(user)\n",
    "    user         = Flatten()(user)\n",
    "    user         = Dense(128)(user)\n",
    "    user         = LeakyReLU()(user)\n",
    "\n",
    "\n",
    "    news         = Conv1D(128,1)(NewsFeatures)\n",
    "    news         = LeakyReLU()(news)\n",
    "    news         = Flatten()(news)\n",
    "    news         = Dense(128)(news)\n",
    "    news         = LeakyReLU()(news)\n",
    "\n",
    "\n",
    "    concate      = Concatenate()([user ,news])\n",
    "\n",
    "    concate      = Dense(256)(concate)\n",
    "    concate      = LeakyReLU()(concate)\n",
    "\n",
    "    out          = Dense(2)(concate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model        = Model(inputs=[UserFeatures,NewsFeatures],outputs=out) \n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "UserStateShape  =(1,5000)\n",
    "NewsStateShape  =(1,5000)\n",
    "\n",
    "model           = create_model( UserStateShape , NewsStateShape )\n",
    "target_model    = create_model( UserStateShape , NewsStateShape )\n",
    "\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "#//////////////  loss_function( y_true , y_predict )\n",
    "loss_function = tf.keras.losses.Huber()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ساخت یک کلاس برای آموزش راحت تر مدل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dqn_model():\n",
    "\n",
    "    def __init__(self,env,model ,target_model ,loss_function ,optimizer ,discount_factor ,action_size= 2):\n",
    "\n",
    "        self.env            = env\n",
    "        self.action_size    = action_size\n",
    "        self.model          = model\n",
    "        self.target_model   = target_model\n",
    "        self.loss_function  = loss_function\n",
    "        self.optimizer      = optimizer\n",
    "        self.discount       = discount_factor\n",
    "        \n",
    "    # private method\n",
    "    #   this method is the core to learn policy\n",
    "    @tf.function\n",
    "    def __train(self,StateUser,StateNews ,action ,reward ,NewStateUser):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            Q1 = self.model([StateUser ,StateNews] ,training=True)\n",
    "            Q2 = Q1[::]\n",
    "\n",
    "            reward = tf.cast(reward ,tf.float32)\n",
    "            Y = reward + self.discount * tf.reduce_max( self.target_model([NewStateUser,StateNews] , training=True) ,axis=-1 )\n",
    "            \n",
    "            \n",
    "            action_index = tf.one_hot([action] , self.action_size)\n",
    "\n",
    "            Q2 = tf.tensor_scatter_nd_update(Q2 , tf.where(action_index)  , Y)\n",
    "            \n",
    "            loss = self.loss_function(Q2 ,Q1)\n",
    "\n",
    "        grad = tape.gradient(loss , self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip( grad , self.model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # private method\n",
    "    #  This method helps to put the above method in a for loop\n",
    "    def __learn(self,StateUser,StateNews ,action ,reward ,NewStateUser ):\n",
    "        \n",
    "        loss = self.__train(StateUser,StateNews ,action ,reward ,NewStateUser)\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    # به روز رسانی شبکه هدف\n",
    "    @tf.function\n",
    "    def soft_Update(self,  target_model_weights    ,  model_weights   ,  tau=0.01):\n",
    "\n",
    "        for TM , M in zip(target_model_weights ,model_weights):\n",
    "            TM.assign(  (1-tau)*TM  +  tau * M   )\n",
    "        \n",
    "\n",
    "    #  این تابع ، با استفاده از  محیط، به شبکه ی عصبی، ویژگی های اخبار مورد علاقه ی و  کاربر مورد نظر را آموزش می دهد\n",
    "    def Learn_favorite_category(self,userid , n_epocks=1 ,early_stopping = 0.01):\n",
    "\n",
    "        transition          = self.env.get_transitions(userid) \n",
    "\n",
    "        StateUser           = transition[\"state\"][np.newaxis,:,:]\n",
    "        NewStateUser        = transition[\"newstate\"][np.newaxis,:,:]\n",
    "        LikedNewsVector     = transition[\"LikedNewsVector\"][np.newaxis,:,:]\n",
    "        DisLikedNewsVector  = transition[\"DisLikedNewsVector\"][np.newaxis,:,:]\n",
    "        ActionForLike       = 1\n",
    "        ActionForDislike    = 0\n",
    "\n",
    "        Reward              = transition[\"Reward\"]\n",
    "        Punishment          = transition[\"Punishment\"]\n",
    "\n",
    "        TotalLoss=[]\n",
    "        for epock in range(n_epocks):\n",
    "            \n",
    "            loss1          = self.__learn(StateUser,LikedNewsVector    ,ActionForLike    ,Reward     ,NewStateUser )\n",
    "            loss2          = self.__learn(StateUser,DisLikedNewsVector ,ActionForDislike ,Punishment ,NewStateUser ,)\n",
    "            \n",
    "            TotalLoss.append(loss1+loss2)\n",
    "            \n",
    "            if (loss1 + loss2)<=early_stopping:\n",
    "                break        \n",
    "                \n",
    "        MeanTotalLoss     = np.mean(TotalLoss)\n",
    "\n",
    "        return MeanTotalLoss\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dqn                     = Dqn_model(env ,model ,target_model ,loss_function ,optimizer,0.9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  debugging for Env and DQN classes......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dqn is learning User689161\n",
      "dqn is learning User187914\n",
      "dqn is learning User161422\n",
      "dqn is learning User644294\n",
      "dqn is learning User126109\n",
      "dqn is learning User347246\n",
      "dqn is learning User374195\n",
      "dqn is learning User675577\n",
      "dqn is learning User511749\n",
      "dqn is learning User332176\n",
      "dqn is learning User165282\n",
      "dqn is learning User473475\n",
      "dqn is learning User489641\n",
      "dqn is learning User413659\n",
      "dqn is learning User110273\n",
      "dqn is learning User171397\n",
      "dqn is learning User68950\n",
      "dqn is learning User55793\n",
      "dqn is learning User188503\n",
      "dqn is learning User403105\n",
      "dqn is learning User157178\n",
      "dqn is learning User129343\n",
      "dqn is learning User70726\n",
      "dqn is learning User603192\n",
      "dqn is learning User65371\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.randint(low =0 ,high = 711221,size= 50):\n",
    "    \n",
    "    #می‌باشد  __train   در بالای تابع   @tf.function    مشاهده شد، مربوط به    warning  اگر در این جا \n",
    "    print(f\"dqn is learning User{i}\")\n",
    "    dqn.Learn_favorite_category(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 in progress.....\n",
      "episode 1 finished. loss = 42.76100540161133 \n",
      " ==================================================\n",
      "Episode 2 in progress.....\n",
      "episode 2 finished. loss = 122.8829116821289 \n",
      " ==================================================\n",
      "Episode 3 in progress.....\n",
      "episode 3 finished. loss = 48.442771911621094 \n",
      " ==================================================\n",
      "Episode 4 in progress.....\n",
      "episode 4 finished. loss = 47.33455276489258 \n",
      " ==================================================\n",
      "Episode 5 in progress.....\n",
      "episode 5 finished. loss = 71.736328125 \n",
      " ==================================================\n",
      "Episode 6 in progress.....\n",
      "episode 6 finished. loss = 146.98751831054688 \n",
      " ==================================================\n",
      "Episode 7 in progress.....\n",
      "episode 7 finished. loss = 100.71116638183594 \n",
      " ==================================================\n",
      "Episode 8 in progress.....\n",
      "episode 8 finished. loss = 158.2576141357422 \n",
      " ==================================================\n",
      "Episode 9 in progress.....\n",
      "episode 9 finished. loss = 93.08625793457031 \n",
      " ==================================================\n",
      "Episode 10 in progress.....\n",
      "episode 10 finished. loss = 222.7868194580078 \n",
      " ==================================================\n"
     ]
    }
   ],
   "source": [
    "n_episodes        = 10   #     تعداد کل مراحل آموزشی \n",
    "length_of_episode = 10   #     در هر بار آموزش شبکه عصبی، چند کاربر به شبکه عصبی آموزش داده میشود\n",
    "\n",
    "n_iterations      = 10   #     علایق هر کاربر، چند مرتبه به شبکه عصبی آموزش داده می‌شود\n",
    "\n",
    "number_of_users   = 711221     #   تعداد کل  کاربران موجود در دیتاست\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "total_loss = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "   print(f\"Episode {episode+1} in progress.....\")\n",
    "\n",
    "   #یاد گرفته شود    end_index    تا کاربر شماره ی     start_index      از کاربر شماره ی    \n",
    "   start_index = np.random.randint( low=0   ,  high= number_of_users -500)\n",
    "   end_index   = start_index + length_of_episode\n",
    "   \n",
    "   episode_loss =[]\n",
    "   for userid in range(start_index , end_index ):\n",
    "      \n",
    "      loss_ = dqn.Learn_favorite_category(userid , n_iterations )\n",
    "\n",
    "      episode_loss.append(loss_)\n",
    "\n",
    "   episode_loss = np.mean(episode_loss)\n",
    "   total_loss.append(episode_loss)\n",
    "\n",
    "      \n",
    "   if episode%10==0:\n",
    "      dqn.soft_Update(dqn.target_model.trainable_variables,dqn.model.trainable_variables )\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "   print(f\"episode {episode +1} finished. loss = {episode_loss} \\n\",\"=\"*50)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "703cc23dfc81a25e43b5bf51938915990284a5a268e2fc63c8ffc4a4768c8d82"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
